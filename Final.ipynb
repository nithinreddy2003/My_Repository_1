{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2af0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "from keras.preprocessing.image import load_img\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2564242f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/All Documents/Project/FMBMRS Using DeepFace/Images/millie.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Millie\u001b[38;5;241m=\u001b[39mload_img(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/All Documents/Project/FMBMRS Using DeepFace/Images/millie.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(Millie\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m500\u001b[39m)))\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py:422\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[0;32m    421\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    423\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/All Documents/Project/FMBMRS Using DeepFace/Images/millie.jpeg'"
     ]
    }
   ],
   "source": [
    "Millie=load_img(\"D:/All Documents/Project/FMBMRS Using DeepFace/Images/millie.jpeg\")\n",
    "plt.imshow(Millie.resize((500, 500)))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc8b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644c772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c3369c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = DeepFace.verify(img1_path = \"D:/All Documents/Project/FMBMRS Using DeepFace/Images/millie_1.jpeg\", img2_path = \"D:/All Documents/Project/FMBMRS Using DeepFace/Images/millie_2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eab3220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verified': True,\n",
       " 'distance': 0.6461677173341298,\n",
       " 'threshold': 0.68,\n",
       " 'model': 'VGG-Face',\n",
       " 'detector_backend': 'opencv',\n",
       " 'similarity_metric': 'cosine',\n",
       " 'facial_areas': {'img1': {'x': 354,\n",
       "   'y': 76,\n",
       "   'w': 312,\n",
       "   'h': 312,\n",
       "   'left_eye': (101, 117),\n",
       "   'right_eye': (212, 125)},\n",
       "  'img2': {'x': 1376,\n",
       "   'y': 405,\n",
       "   'w': 885,\n",
       "   'h': 885,\n",
       "   'left_eye': (303, 325),\n",
       "   'right_eye': (630, 390)}},\n",
       " 'time': 4.88}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0585c144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified: True\n",
      "Distance: 0.6461677173341298\n",
      "Threshold: 0.68\n",
      "Model: VGG-Face\n",
      "Similarity Metric: cosine\n"
     ]
    }
   ],
   "source": [
    "print(\"Verified:\", result['verified'])\n",
    "print(\"Distance:\", result['distance'])\n",
    "print(\"Threshold:\", result['threshold'])\n",
    "print(\"Model:\", result['model'])\n",
    "print(\"Similarity Metric:\", result['similarity_metric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16c7dbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-03-19 12:07:36 - Found 3 new images and 0 removed images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding representations: 100%|███████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-03-19 12:07:41 - There are now 7 representations in representations_vgg_face.pkl\n",
      "24-03-19 12:07:42 - find function duration 6.039804697036743 seconds\n"
     ]
    }
   ],
   "source": [
    "dfs = DeepFace.find(img_path = \"D:/All Documents/Project/FMBMRS Using DeepFace/Images/millie_1.jpeg\", db_path = \"D:/All Documents/Project/FMBMRS Using DeepFace/Images/Mil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19078751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                            identity  target_x  target_y  \\\n",
       " 0  D:/All Documents/Project/FMBMRS Using DeepFace...       296        63   \n",
       " 1  D:/All Documents/Project/FMBMRS Using DeepFace...       506       181   \n",
       " \n",
       "    target_w  target_h  source_x  source_y  source_w  source_h  threshold  \\\n",
       " 0       158       158       354        76       312       312       0.68   \n",
       " 1       131       131       354        76       312       312       0.68   \n",
       " \n",
       "    distance  \n",
       " 0  0.515026  \n",
       " 1  0.641412  ]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfe27942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|███████████████████████████████████████████████████████████████████| 4/4 [00:25<00:00,  6.36s/it]\n"
     ]
    }
   ],
   "source": [
    "objs = DeepFace.analyze(img_path = \"D:/All Documents/Project/FMBMRS Using DeepFace/Images/millie_1.jpeg\", \n",
    "        actions = ['age', 'gender', 'race', 'emotion']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c949bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'age': 33,\n",
       "  'region': {'x': 354,\n",
       "   'y': 76,\n",
       "   'w': 312,\n",
       "   'h': 312,\n",
       "   'left_eye': (101, 117),\n",
       "   'right_eye': (212, 125)},\n",
       "  'face_confidence': 0.91,\n",
       "  'gender': {'Woman': 99.98542070388794, 'Man': 0.01457546604797244},\n",
       "  'dominant_gender': 'Woman',\n",
       "  'race': {'asian': 0.00010367180656815937,\n",
       "   'indian': 4.560031060324214e-05,\n",
       "   'black': 9.127862565847956e-07,\n",
       "   'white': 99.60405228162124,\n",
       "   'middle eastern': 0.1408476582861041,\n",
       "   'latino hispanic': 0.2549514559254918},\n",
       "  'dominant_race': 'white',\n",
       "  'emotion': {'angry': 0.19339959835633636,\n",
       "   'disgust': 9.185349654217134e-05,\n",
       "   'fear': 1.1713606305420399,\n",
       "   'happy': 11.319126933813095,\n",
       "   'sad': 2.0714113488793373,\n",
       "   'surprise': 0.0684658472891897,\n",
       "   'neutral': 85.17614006996155},\n",
       "  'dominant_emotion': 'neutral'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82dda8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|███████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "objs = DeepFace.analyze(img_path = \"D:/All Documents/Project/FMBMRS Using DeepFace/Images/Mil/millie_6.png\", \n",
    "        actions = ['age', 'gender', 'race', 'emotion']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e01c205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'age': 33,\n",
       "  'region': {'x': 664,\n",
       "   'y': 128,\n",
       "   'w': 296,\n",
       "   'h': 296,\n",
       "   'left_eye': None,\n",
       "   'right_eye': None},\n",
       "  'face_confidence': 0.95,\n",
       "  'gender': {'Woman': 2.418357878923416, 'Man': 97.58164286613464},\n",
       "  'dominant_gender': 'Man',\n",
       "  'race': {'asian': 5.109691992402077,\n",
       "   'indian': 25.6475567817688,\n",
       "   'black': 33.090510964393616,\n",
       "   'white': 8.051449060440063,\n",
       "   'middle eastern': 6.249075382947922,\n",
       "   'latino hispanic': 21.851715445518494},\n",
       "  'dominant_race': 'black',\n",
       "  'emotion': {'angry': 94.6921176085332,\n",
       "   'disgust': 1.373277068343512,\n",
       "   'fear': 1.3741246649176282,\n",
       "   'happy': 0.013883480078461161,\n",
       "   'sad': 2.545399660420458,\n",
       "   'surprise': 2.856937812144176e-06,\n",
       "   'neutral': 0.0011986143753303774},\n",
       "  'dominant_emotion': 'angry'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e32b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|███████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "objs = DeepFace.analyze(img_path = \"D:/All Documents/Project/FMBMRS Using DeepFace/Images/Mil/millie_10.jpg\", \n",
    "        actions = ['age', 'gender', 'race', 'emotion']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96989325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'age': 27,\n",
       "  'region': {'x': 845,\n",
       "   'y': 181,\n",
       "   'w': 366,\n",
       "   'h': 366,\n",
       "   'left_eye': (114, 142),\n",
       "   'right_eye': (249, 138)},\n",
       "  'face_confidence': 0.89,\n",
       "  'gender': {'Woman': 68.55156421661377, 'Man': 31.44843876361847},\n",
       "  'dominant_gender': 'Woman',\n",
       "  'race': {'asian': 0.00035272892091597896,\n",
       "   'indian': 0.00015118628198251827,\n",
       "   'black': 2.6086496518473723e-06,\n",
       "   'white': 99.19580221176147,\n",
       "   'middle eastern': 0.42716911993920803,\n",
       "   'latino hispanic': 0.37651986349374056},\n",
       "  'dominant_race': 'white',\n",
       "  'emotion': {'angry': 11.227648705244064,\n",
       "   'disgust': 5.944912828681481e-06,\n",
       "   'fear': 1.4340607449412346,\n",
       "   'happy': 0.0009252324161934666,\n",
       "   'sad': 24.040307104587555,\n",
       "   'surprise': 4.496947383358929e-05,\n",
       "   'neutral': 63.29700946807861},\n",
       "  'dominant_emotion': 'neutral'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f1d1e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|███████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "objs = DeepFace.analyze(img_path = \"D:/All Documents/Project/FMBMRS Using DeepFace/Images/Mil/millie_11.jpg\", \n",
    "        actions = ['age', 'gender', 'race', 'emotion']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa32f740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'age': 33,\n",
       "  'region': {'x': 314,\n",
       "   'y': 87,\n",
       "   'w': 227,\n",
       "   'h': 227,\n",
       "   'left_eye': (76, 80),\n",
       "   'right_eye': (161, 89)},\n",
       "  'face_confidence': 0.92,\n",
       "  'gender': {'Woman': 99.99991655349731, 'Man': 8.697667226442718e-05},\n",
       "  'dominant_gender': 'Woman',\n",
       "  'race': {'asian': 0.00014643145505033317,\n",
       "   'indian': 1.7151414510863106e-05,\n",
       "   'black': 8.097102432991221e-08,\n",
       "   'white': 99.89712238311768,\n",
       "   'middle eastern': 0.045025203144177794,\n",
       "   'latino hispanic': 0.05768941482529044},\n",
       "  'dominant_race': 'white',\n",
       "  'emotion': {'angry': 7.382096707612473e-07,\n",
       "   'disgust': 4.689388551328327e-13,\n",
       "   'fear': 2.168860997952924e-07,\n",
       "   'happy': 99.99987483023851,\n",
       "   'sad': 4.0245745248276286e-08,\n",
       "   'surprise': 0.00011544794466862777,\n",
       "   'neutral': 1.1638088875737405e-05},\n",
       "  'dominant_emotion': 'happy'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3b032c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-03-19 15:06:00 - facial recognition model VGG-Face is just built\n",
      "24-03-19 15:06:00 - Age model is just built\n",
      "24-03-19 15:06:00 - Gender model is just built\n",
      "24-03-19 15:06:00 - Emotion model is just built\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Passed db_path does not exist!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m DeepFace\u001b[38;5;241m.\u001b[39mstream(db_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/All Documents/Project/FMBMRS Using DeepFace/Images/zum.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\DeepFace.py:412\u001b[0m, in \u001b[0;36mstream\u001b[1;34m(db_path, model_name, detector_backend, distance_metric, enable_face_analysis, source, time_threshold, frame_threshold)\u001b[0m\n\u001b[0;32m    409\u001b[0m time_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(time_threshold, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    410\u001b[0m frame_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(frame_threshold, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 412\u001b[0m streaming\u001b[38;5;241m.\u001b[39manalysis(\n\u001b[0;32m    413\u001b[0m     db_path\u001b[38;5;241m=\u001b[39mdb_path,\n\u001b[0;32m    414\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    415\u001b[0m     detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m    416\u001b[0m     distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric,\n\u001b[0;32m    417\u001b[0m     enable_face_analysis\u001b[38;5;241m=\u001b[39menable_face_analysis,\n\u001b[0;32m    418\u001b[0m     source\u001b[38;5;241m=\u001b[39msource,\n\u001b[0;32m    419\u001b[0m     time_threshold\u001b[38;5;241m=\u001b[39mtime_threshold,\n\u001b[0;32m    420\u001b[0m     frame_threshold\u001b[38;5;241m=\u001b[39mframe_threshold,\n\u001b[0;32m    421\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\modules\\streaming.py:53\u001b[0m, in \u001b[0;36manalysis\u001b[1;34m(db_path, model_name, detector_backend, distance_metric, enable_face_analysis, source, time_threshold, frame_threshold)\u001b[0m\n\u001b[0;32m     50\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmotion model is just built\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# call a dummy find function for db_path once to create embeddings in the initialization\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m DeepFace\u001b[38;5;241m.\u001b[39mfind(\n\u001b[0;32m     54\u001b[0m     img_path\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m]),\n\u001b[0;32m     55\u001b[0m     db_path\u001b[38;5;241m=\u001b[39mdb_path,\n\u001b[0;32m     56\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m     57\u001b[0m     detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m     58\u001b[0m     distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric,\n\u001b[0;32m     59\u001b[0m     enforce_detection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# visualization\u001b[39;00m\n\u001b[0;32m     63\u001b[0m freeze \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\DeepFace.py:294\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(img_path, db_path, model_name, distance_metric, enforce_detection, detector_backend, align, expand_percentage, threshold, normalization, silent)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind\u001b[39m(\n\u001b[0;32m    227\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m    228\u001b[0m     db_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m     silent: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[pd\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m    239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    Identify individuals in a database\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m                specified model and distance metric\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recognition\u001b[38;5;241m.\u001b[39mfind(\n\u001b[0;32m    295\u001b[0m         img_path\u001b[38;5;241m=\u001b[39mimg_path,\n\u001b[0;32m    296\u001b[0m         db_path\u001b[38;5;241m=\u001b[39mdb_path,\n\u001b[0;32m    297\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    298\u001b[0m         distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric,\n\u001b[0;32m    299\u001b[0m         enforce_detection\u001b[38;5;241m=\u001b[39menforce_detection,\n\u001b[0;32m    300\u001b[0m         detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m    301\u001b[0m         align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m    302\u001b[0m         expand_percentage\u001b[38;5;241m=\u001b[39mexpand_percentage,\n\u001b[0;32m    303\u001b[0m         threshold\u001b[38;5;241m=\u001b[39mthreshold,\n\u001b[0;32m    304\u001b[0m         normalization\u001b[38;5;241m=\u001b[39mnormalization,\n\u001b[0;32m    305\u001b[0m         silent\u001b[38;5;241m=\u001b[39msilent,\n\u001b[0;32m    306\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\modules\\recognition.py:93\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(img_path, db_path, model_name, distance_metric, enforce_detection, detector_backend, align, expand_percentage, threshold, normalization, silent)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(db_path) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed db_path does not exist!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m model: FacialRecognition \u001b[38;5;241m=\u001b[39m modeling\u001b[38;5;241m.\u001b[39mbuild_model(model_name)\n\u001b[0;32m     96\u001b[0m target_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minput_shape\n",
      "\u001b[1;31mValueError\u001b[0m: Passed db_path does not exist!"
     ]
    }
   ],
   "source": [
    "DeepFace.stream(db_path = \"D:/All Documents/Project/FMBMRS Using DeepFace/Images/zum.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f13df0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-03-19 15:16:33 - facial recognition model VGG-Face is just built\n",
      "24-03-19 15:16:33 - Age model is just built\n",
      "24-03-19 15:16:33 - Gender model is just built\n",
      "24-03-19 15:16:33 - Emotion model is just built\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Passed db_path does not exist!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     DeepFace\u001b[38;5;241m.\u001b[39mstream()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Call the function to start real-time analysis\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m real_time_analysis()\n",
      "Cell \u001b[1;32mIn[53], line 6\u001b[0m, in \u001b[0;36mreal_time_analysis\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreal_time_analysis\u001b[39m():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Start real-time analysis on webcam video stream\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     DeepFace\u001b[38;5;241m.\u001b[39mstream()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\DeepFace.py:412\u001b[0m, in \u001b[0;36mstream\u001b[1;34m(db_path, model_name, detector_backend, distance_metric, enable_face_analysis, source, time_threshold, frame_threshold)\u001b[0m\n\u001b[0;32m    409\u001b[0m time_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(time_threshold, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    410\u001b[0m frame_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(frame_threshold, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 412\u001b[0m streaming\u001b[38;5;241m.\u001b[39manalysis(\n\u001b[0;32m    413\u001b[0m     db_path\u001b[38;5;241m=\u001b[39mdb_path,\n\u001b[0;32m    414\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    415\u001b[0m     detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m    416\u001b[0m     distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric,\n\u001b[0;32m    417\u001b[0m     enable_face_analysis\u001b[38;5;241m=\u001b[39menable_face_analysis,\n\u001b[0;32m    418\u001b[0m     source\u001b[38;5;241m=\u001b[39msource,\n\u001b[0;32m    419\u001b[0m     time_threshold\u001b[38;5;241m=\u001b[39mtime_threshold,\n\u001b[0;32m    420\u001b[0m     frame_threshold\u001b[38;5;241m=\u001b[39mframe_threshold,\n\u001b[0;32m    421\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\modules\\streaming.py:53\u001b[0m, in \u001b[0;36manalysis\u001b[1;34m(db_path, model_name, detector_backend, distance_metric, enable_face_analysis, source, time_threshold, frame_threshold)\u001b[0m\n\u001b[0;32m     50\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmotion model is just built\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# call a dummy find function for db_path once to create embeddings in the initialization\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m DeepFace\u001b[38;5;241m.\u001b[39mfind(\n\u001b[0;32m     54\u001b[0m     img_path\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m]),\n\u001b[0;32m     55\u001b[0m     db_path\u001b[38;5;241m=\u001b[39mdb_path,\n\u001b[0;32m     56\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m     57\u001b[0m     detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m     58\u001b[0m     distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric,\n\u001b[0;32m     59\u001b[0m     enforce_detection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# visualization\u001b[39;00m\n\u001b[0;32m     63\u001b[0m freeze \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\DeepFace.py:294\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(img_path, db_path, model_name, distance_metric, enforce_detection, detector_backend, align, expand_percentage, threshold, normalization, silent)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind\u001b[39m(\n\u001b[0;32m    227\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m    228\u001b[0m     db_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m     silent: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[pd\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m    239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    Identify individuals in a database\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m                specified model and distance metric\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recognition\u001b[38;5;241m.\u001b[39mfind(\n\u001b[0;32m    295\u001b[0m         img_path\u001b[38;5;241m=\u001b[39mimg_path,\n\u001b[0;32m    296\u001b[0m         db_path\u001b[38;5;241m=\u001b[39mdb_path,\n\u001b[0;32m    297\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    298\u001b[0m         distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric,\n\u001b[0;32m    299\u001b[0m         enforce_detection\u001b[38;5;241m=\u001b[39menforce_detection,\n\u001b[0;32m    300\u001b[0m         detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m    301\u001b[0m         align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m    302\u001b[0m         expand_percentage\u001b[38;5;241m=\u001b[39mexpand_percentage,\n\u001b[0;32m    303\u001b[0m         threshold\u001b[38;5;241m=\u001b[39mthreshold,\n\u001b[0;32m    304\u001b[0m         normalization\u001b[38;5;241m=\u001b[39mnormalization,\n\u001b[0;32m    305\u001b[0m         silent\u001b[38;5;241m=\u001b[39msilent,\n\u001b[0;32m    306\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\modules\\recognition.py:93\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(img_path, db_path, model_name, distance_metric, enforce_detection, detector_backend, align, expand_percentage, threshold, normalization, silent)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(db_path) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed db_path does not exist!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m model: FacialRecognition \u001b[38;5;241m=\u001b[39m modeling\u001b[38;5;241m.\u001b[39mbuild_model(model_name)\n\u001b[0;32m     96\u001b[0m target_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minput_shape\n",
      "\u001b[1;31mValueError\u001b[0m: Passed db_path does not exist!"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "# Function to perform real-time analysis on webcam video stream\n",
    "def real_time_analysis():\n",
    "    # Start real-time analysis on webcam video stream\n",
    "    DeepFace.stream()\n",
    "\n",
    "# Call the function to start real-time analysis\n",
    "real_time_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b440209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-03-19 15:17:40 - facial recognition model VGG-Face is just built\n",
      "24-03-19 15:17:40 - Age model is just built\n",
      "24-03-19 15:17:40 - Gender model is just built\n",
      "24-03-19 15:17:40 - Emotion model is just built\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Passed db_path does not exist!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Call the function to start real-time analysis and save the video\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m real_time_analysis_with_save(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_video.avi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[55], line 21\u001b[0m, in \u001b[0;36mreal_time_analysis_with_save\u001b[1;34m(video_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m out\u001b[38;5;241m.\u001b[39mwrite(frame)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Perform real-time analysis on the frame\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m DeepFace\u001b[38;5;241m.\u001b[39mstream(frame)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Display the resulting frame\u001b[39;00m\n\u001b[0;32m     24\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrame\u001b[39m\u001b[38;5;124m'\u001b[39m,frame)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\DeepFace.py:412\u001b[0m, in \u001b[0;36mstream\u001b[1;34m(db_path, model_name, detector_backend, distance_metric, enable_face_analysis, source, time_threshold, frame_threshold)\u001b[0m\n\u001b[0;32m    409\u001b[0m time_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(time_threshold, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    410\u001b[0m frame_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(frame_threshold, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 412\u001b[0m streaming\u001b[38;5;241m.\u001b[39manalysis(\n\u001b[0;32m    413\u001b[0m     db_path\u001b[38;5;241m=\u001b[39mdb_path,\n\u001b[0;32m    414\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    415\u001b[0m     detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m    416\u001b[0m     distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric,\n\u001b[0;32m    417\u001b[0m     enable_face_analysis\u001b[38;5;241m=\u001b[39menable_face_analysis,\n\u001b[0;32m    418\u001b[0m     source\u001b[38;5;241m=\u001b[39msource,\n\u001b[0;32m    419\u001b[0m     time_threshold\u001b[38;5;241m=\u001b[39mtime_threshold,\n\u001b[0;32m    420\u001b[0m     frame_threshold\u001b[38;5;241m=\u001b[39mframe_threshold,\n\u001b[0;32m    421\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\modules\\streaming.py:53\u001b[0m, in \u001b[0;36manalysis\u001b[1;34m(db_path, model_name, detector_backend, distance_metric, enable_face_analysis, source, time_threshold, frame_threshold)\u001b[0m\n\u001b[0;32m     50\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmotion model is just built\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# call a dummy find function for db_path once to create embeddings in the initialization\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m DeepFace\u001b[38;5;241m.\u001b[39mfind(\n\u001b[0;32m     54\u001b[0m     img_path\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m]),\n\u001b[0;32m     55\u001b[0m     db_path\u001b[38;5;241m=\u001b[39mdb_path,\n\u001b[0;32m     56\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m     57\u001b[0m     detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m     58\u001b[0m     distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric,\n\u001b[0;32m     59\u001b[0m     enforce_detection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# visualization\u001b[39;00m\n\u001b[0;32m     63\u001b[0m freeze \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\DeepFace.py:294\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(img_path, db_path, model_name, distance_metric, enforce_detection, detector_backend, align, expand_percentage, threshold, normalization, silent)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind\u001b[39m(\n\u001b[0;32m    227\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m    228\u001b[0m     db_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m     silent: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[pd\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m    239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    Identify individuals in a database\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m                specified model and distance metric\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recognition\u001b[38;5;241m.\u001b[39mfind(\n\u001b[0;32m    295\u001b[0m         img_path\u001b[38;5;241m=\u001b[39mimg_path,\n\u001b[0;32m    296\u001b[0m         db_path\u001b[38;5;241m=\u001b[39mdb_path,\n\u001b[0;32m    297\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    298\u001b[0m         distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric,\n\u001b[0;32m    299\u001b[0m         enforce_detection\u001b[38;5;241m=\u001b[39menforce_detection,\n\u001b[0;32m    300\u001b[0m         detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m    301\u001b[0m         align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m    302\u001b[0m         expand_percentage\u001b[38;5;241m=\u001b[39mexpand_percentage,\n\u001b[0;32m    303\u001b[0m         threshold\u001b[38;5;241m=\u001b[39mthreshold,\n\u001b[0;32m    304\u001b[0m         normalization\u001b[38;5;241m=\u001b[39mnormalization,\n\u001b[0;32m    305\u001b[0m         silent\u001b[38;5;241m=\u001b[39msilent,\n\u001b[0;32m    306\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\modules\\recognition.py:93\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(img_path, db_path, model_name, distance_metric, enforce_detection, detector_backend, align, expand_percentage, threshold, normalization, silent)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(db_path) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed db_path does not exist!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m model: FacialRecognition \u001b[38;5;241m=\u001b[39m modeling\u001b[38;5;241m.\u001b[39mbuild_model(model_name)\n\u001b[0;32m     96\u001b[0m target_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minput_shape\n",
      "\u001b[1;31mValueError\u001b[0m: Passed db_path does not exist!"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Function to perform real-time analysis on webcam video stream and save the video\n",
    "def real_time_analysis_with_save(video_path):\n",
    "    # Open webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 20.0, (640, 480))\n",
    "    \n",
    "    # Start real-time analysis on webcam video stream\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            # Write the frame into the video file\n",
    "            out.write(frame)\n",
    "            \n",
    "            # Perform real-time analysis on the frame\n",
    "            DeepFace.stream(frame)\n",
    "            \n",
    "            # Display the resulting frame\n",
    "            cv2.imshow('Frame',frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    # Release everything when job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function to start real-time analysis and save the video\n",
    "real_time_analysis_with_save(\"output_video.avi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae11958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
